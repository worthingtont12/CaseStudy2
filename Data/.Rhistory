washington <- read.csv('Washington_cleaned.csv')
texas <- read.csv('Texas_cleaned.csv')
texas <- texas[-2]
texas <- texas[-3]
texas <- texas[-3]
texas <- texas[-6]
texas <- texas[-6]
texas <- texas[-1]
texas <- unique(texas)
washington <- washington[-2]
washington <- washington[-3]
washington <- washington[-3]
washington <- washington[-6]
washington <- washington[-6]
washington <- washington[-1]
washington <- unique(washington)
seattle <- washington[grepl('seattle', washington$user.location, ignore.case = TRUE),]
washington.other <- washington[!grepl('seattle', washington$user.location, ignore.case = TRUE) &
(grepl('WA', washington$user.location) |
grepl('washington', washington$user.location, ignore.case = TRUE)), ]
austin <- texas[grepl('austin', texas$user.location, ignore.case = TRUE),]
dallas <- texas[grepl('dallas', texas$user.location, ignore.case = TRUE),]
houston <- texas[grepl('houston', texas$user.location, ignore.case = TRUE),]
texas.other <- texas[! (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) &
(grepl('TX', texas$user.location) |
grepl('texas', texas$user.location, ignore.case = TRUE)), ]
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("topicmodels", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
detach("package:tm", unload=TRUE)
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages('tm')
install.packages('topicmodels')
library(XML)
library(tm)
library(topicmodels)
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text.clean[[2]]$content
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
library(XML)
library(tm)
library(topicmodels)
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
install.packages('tm')
install.packages("tm")
library(topicmodels)
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
library(XML)
library(tm)
library(topicmodels)
# clean and compute tfidf
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
library("slam", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library(XML)
library(tm)
library(topicmodels)
# clean and compute tfidf
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
install.packages('SnowballC')
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text.clean[[2]]$content
washington <- read.csv('Washington_cleaned.csv')
texas <- read.csv('Texas_cleaned.csv')
texas <- texas[-2]
texas <- texas[-3]
texas <- texas[-3]
texas <- texas[-6]
texas <- texas[-6]
texas <- texas[-1]
texas <- unique(texas)
washington <- washington[-2]
washington <- washington[-3]
washington <- washington[-3]
washington <- washington[-6]
washington <- washington[-6]
washington <- washington[-1]
washington <- unique(washington)
seattle <- washington[grepl('seattle', washington$user.location, ignore.case = TRUE),]
washington.other <- washington[!grepl('seattle', washington$user.location, ignore.case = TRUE) &
(grepl('WA', washington$user.location) |
grepl('washington', washington$user.location, ignore.case = TRUE)), ]
austin <- texas[grepl('austin', texas$user.location, ignore.case = TRUE),]
dallas <- texas[grepl('dallas', texas$user.location, ignore.case = TRUE),]
houston <- texas[grepl('houston', texas$user.location, ignore.case = TRUE),]
texas.other <- texas[! (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) &
(grepl('TX', texas$user.location) |
grepl('texas', texas$user.location, ignore.case = TRUE)), ]
library(XML)
library(tm)
library(topicmodels)
library(SnowballC)
# clean and compute tfidf
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean[[2]]$content
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
# clean Seattle data
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
# clean Washington.Other data
washington.other.text <- VCorpus(DataframeSource(as.data.frame(washington.other$author.text)))
washington.other.text.clean = tm_map(washington.other.text, stripWhitespace)                          # remove extra whitespace
washington.other.text.clean = tm_map(washington.other.text.clean, removeNumbers)                      # remove numbers
washington.other.text.clean = tm_map(washington.other.text.clean, removePunctuation)                  # remove punctuation
washington.other.text.clean = tm_map(washington.other.text.clean, content_transformer(tolower))       # ignore case
washington.other.text.clean = tm_map(washington.other.text.clean, removeWords, stopwords("english"))  # remove stop words
washington.other.text.clean = tm_map(washington.other.text.clean, stemDocument, lazy = TRUE)          # stem all words
washington.other.text.clean.tf = DocumentTermMatrix(washington.other.text.clean, control = list(weighting = weightTf))
# clean Houston data
houston.text <- VCorpus(DataframeSource(as.data.frame(houston$author.text)))
houston.text.clean = tm_map(houston.text, stripWhitespace)                          # remove extra whitespace
houston.text.clean = tm_map(houston.text.clean, removeNumbers)                      # remove numbers
houston.text.clean = tm_map(houston.text.clean, removePunctuation)                  # remove punctuation
houston.text.clean = tm_map(houston.text.clean, content_transformer(tolower))       # ignore case
houston.text.clean = tm_map(houston.text.clean, removeWords, stopwords("english"))  # remove stop words
houston.text.clean = tm_map(houston.text.clean, stemDocument, lazy = TRUE)          # stem all words
houston.text.clean.tf = DocumentTermMatrix(houston.text.clean, control = list(weighting = weightTf))
# clean Austin data
austin.text <- VCorpus(DataframeSource(as.data.frame(austin$author.text)))
austin.text.clean = tm_map(austin.text, stripWhitespace)                          # remove extra whitespace
austin.text.clean = tm_map(austin.text.clean, removeNumbers)                      # remove numbers
austin.text.clean = tm_map(austin.text.clean, removePunctuation)                  # remove punctuation
austin.text.clean = tm_map(austin.text.clean, content_transformer(tolower))       # ignore case
austin.text.clean = tm_map(austin.text.clean, removeWords, stopwords("english"))  # remove stop words
austin.text.clean = tm_map(austin.text.clean, stemDocument, lazy = TRUE)          # stem all words
austin.text.clean.tf = DocumentTermMatrix(austin.text.clean, control = list(weighting = weightTf))
# clean Dallas data
dallas.text <- VCorpus(DataframeSource(as.data.frame(dallas$author.text)))
dallas.text.clean = tm_map(dallas.text, stripWhitespace)                          # remove extra whitespace
dallas.text.clean = tm_map(dallas.text.clean, removeNumbers)                      # remove numbers
dallas.text.clean = tm_map(dallas.text.clean, removePunctuation)                  # remove punctuation
dallas.text.clean = tm_map(dallas.text.clean, content_transformer(tolower))       # ignore case
dallas.text.clean = tm_map(dallas.text.clean, removeWords, stopwords("english"))  # remove stop words
dallas.text.clean = tm_map(dallas.text.clean, stemDocument, lazy = TRUE)          # stem all words
dallas.text.clean.tf = DocumentTermMatrix(dallas.text.clean, control = list(weighting = weightTf))
# clean Texas.Other data
texas.other.text <- VCorpus(DataframeSource(as.data.frame(texas.other$author.text)))
texas.other.text.clean = tm_map(texas.other.text, stripWhitespace)                          # remove extra whitespace
texas.other.text.clean = tm_map(texas.other.text.clean, removeNumbers)                      # remove numbers
texas.other.text.clean = tm_map(texas.other.text.clean, removePunctuation)                  # remove punctuation
texas.other.text.clean = tm_map(texas.other.text.clean, content_transformer(tolower))       # ignore case
texas.other.text.clean = tm_map(texas.other.text.clean, removeWords, stopwords("english"))  # remove stop words
texas.other.text.clean = tm_map(texas.other.text.clean, stemDocument, lazy = TRUE)          # stem all words
texas.other.text.clean.tf = DocumentTermMatrix(texas.other.text.clean, control = list(weighting = weightTf))
row.sums = apply(seattle.text.clean.tf, 1, sum)
topic.model = LDA(seattle.text.clean.tf, 50)
terms(topic.model, 10)[,1:5]
topic.model = LDA(seattle.text.clean.tf, 20)
terms(topic.model, 10)[,1:5]
topics(topic.model, 10)[,1:5]
document.most.likely.topic = topics(topic.model, 1)
document.topic.clusters = split(news, document.most.likely.topic)
topic.space.clustered.news = split(seattle.text.clean.tf, topic.space.kmeans.clusters$cluster)
topic.space.kmeans.clusters = kmeans(document.topic.probabilities, 10)
document.topic.probabilities = topic.model@gamma  # topic distribution for each document
topic.space.kmeans.clusters = kmeans(document.topic.probabilities, 10)
topic.space.clustered.news = split(seattle.text.clean.tf, topic.space.kmeans.clusters$cluster)
topic.space.clustered.news[[1]][[1]]$content
topic.model = LDA(seattle.text.clean.tf, 10)
terms(topic.model, 10)[,1:5]
terms(topic.model, 10)[,1:10]
topic.model = LDA(seattle.text.clean.tf, 5)
terms(topic.model, 10)[,1:5]
topic.model = LDA(seattle.text.clean.tf, 10)
terms(topic.model, 10)[,1:10]
topic.model = LDA(seattle.text.clean.tf, 7)
terms(topic.model, 10)[,1:7]
topic.model = LDA(seattle.text.clean.tf, 5)
terms(topic.model, 10)[,1:5]
topic.model = LDA(seattle.text.clean.tf, 10)
terms(topic.model, 10)[,1:10]
topics(topic.model, 10)[,1:5]
terms(topic.model, 10)[,1:10]
View(seattle)
topics(topic.model, 10)[,1:5]
topic.model = LDA(washington.other.text.clean.tf, 10)
terms(topic.model, 10)[,1:10]
topic.model = LDA(dallas.text.clean.tf, 10)
terms(topic.model, 10)[,1:10]
topic.model = LDA(dallas.text.clean.tf, 5)
terms(topic.model, 10)[,1:5]
topic.model = LDA(austin.text.clean.tf, 5)
terms(topic.model, 10)[,1:5]
terms(topic.model, 5)[,1:5]
topic.model = LDA(austin.text.clean.tf, 5)
terms(topic.model, 10)[,1:5]
topics(topic.model, 10)[,1:5]
document.most.likely.topic = topics(topic.model, 1)
document.topic.clusters = split(, document.most.likely.topic)
document.topic.clusters = split(austin, document.most.likely.topic)
document.topic.clusters[[3]][[1]]$content
set.seed(1)
news = as.data.frame(xmlToDataFrame("Data/news_documents.xml", stringsAsFactors = FALSE)[,"c"])
news = as.data.frame(xmlToDataFrame("~/MSDS/SYS6018/Examples/Data/news_documents.xml", stringsAsFactors = FALSE)[,"c"])
news = VCorpus(DataframeSource(news))
news = news[sample(1:length(news), 100)]
# clean and compute tfidf
news.clean = tm_map(news, stripWhitespace)                          # remove extra whitespace
news.clean = tm_map(news.clean, removeNumbers)                      # remove numbers
news.clean = tm_map(news.clean, removePunctuation)                  # remove punctuation
news.clean = tm_map(news.clean, content_transformer(tolower))       # ignore case
news.clean = tm_map(news.clean, removeWords, stopwords("english"))  # remove stop words
news.clean = tm_map(news.clean, stemDocument)                       # stem all words
news.clean.tf = DocumentTermMatrix(news.clean, control = list(weighting = weightTf))
# remove empty documents
row.sums = apply(news.clean.tf, 1, sum)
news = news[row.sums > 0]
news.clean.tf = news.clean.tf[row.sums > 0,]
# train topic model with 50 topics
topic.model = LDA(news.clean.tf, 50)
terms(topic.model, 10)[,1:5]
topics(topic.model, 10)[,1:5]
document.most.likely.topic = topics(topic.model, 1)
document.topic.clusters = split(news, document.most.likely.topic)
document.topic.clusters[[3]][[1]]$content
document.topic.clusters[[3]][[2]]$content
document.topic.clusters[[3]][[3]]$content
# gamma contains the document-topic matrix
topic.model@gamma[1:5,]
document.topic.probabilities = topic.model@gamma  # topic distribution for each document
topic.space.kmeans.clusters = kmeans(document.topic.probabilities, 10)
topic.space.clustered.news = split(news, topic.space.kmeans.clusters$cluster)
topic.space.clustered.news[[1]][[1]]$content
topic.space.clustered.news[[1]][[2]]$content
topic.space.clustered.news[[1]][[3]]$content
news.clean.tf = news.clean.tf[row.sums > 0,]
topic.model = LDA(austin.text.clean.tf, 5)
terms(topic.model, 10)[,1:10]
terms(topic.model, 10)[,1:5]
View(texas.other)
sanantonio<-texas[grepl('San Antonio', texas$user.location, ignore.case = TRUE),]
texas.other <- texas[! (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) |
grepl('San Antonio', texas$user.location, ignore.case = TRUE) &
(grepl('TX', texas$user.location) |
grepl('texas', texas$user.location, ignore.case = TRUE)), ]
san.text <- VCorpus(DataframeSource(as.data.frame(sanantonio$author.text)))
san.text.clean = tm_map(san.text, stripWhitespace)                          # remove extra whitespace
san.text.clean = tm_map(san.text.clean, removeNumbers)                      # remove numbers
san.text.clean = tm_map(san.text.clean, removePunctuation)                  # remove punctuation
san.text.clean = tm_map(san.text.clean, content_transformer(tolower))       # ignore case
san.text.clean = tm_map(san.text.clean, removeWords, stopwords("english"))  # remove stop words
san.text.clean = tm_map(san.text.clean, stemDocument, lazy = TRUE)          # stem all words
san.text.clean.tf = DocumentTermMatrix(san.text.clean, control = list(weighting = weightTf))
texas.cities <-texas[ (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) |
grepl('San Antonio', texas$user.location, ignore.case = TRUE) ]
texas.cities <-texas[ (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) |
grepl('San Antonio', texas$user.location, ignore.case = TRUE), ]
tex.cities.text <- VCorpus(DataframeSource(as.data.frame(texas.cities$author.text)))
tex.cities.clean = tm_map(tex.cities.text, stripWhitespace)                          # remove extra whitespace
tex.cities.clean = tm_map(tex.cities.clean, removeNumbers)                      # remove numbers
tex.cities.clean = tm_map(tex.cities.clean, removePunctuation)                  # remove punctuation
tex.cities.clean = tm_map(tex.cities.clean, content_transformer(tolower))       # ignore case
tex.cities.clean = tm_map(tex.cities.clean, removeWords, stopwords("english"))  # remove stop words
tex.cities.clean = tm_map(tex.cities.clean, stemDocument, lazy = TRUE)          # stem all words
tex.cities.clean.tf = DocumentTermMatrix(tex.cities.clean, control = list(weighting = weightTf))
topic.model = LDA(tex.cities.clean.tf, 10)
terms(topic.model, 10)[,1:5]
terms(topic.model, 10)[,1:10]
topic.model = LDA(texas.other.text.clean.tf, 10)
topic.model = LDA(texas.other.text.clean.tf, 10)
topic.model = LDA(texas.other.text.clean.tf, 10)
row.sums = apply(texas.other.text.clean.tf, 1, sum)
texas.other = texas.other[row.sums > 0]
texas.other.text.clean.tf = texas.other.text.clean.tf[row.sums > 0,]
topic.model = LDA(texas.other.text.clean.tf, 10)
terms(topic.model, 10)[,1:10]
clearPushBack()
washington <- read.csv('Washington_cleaned.csv')
texas <- read.csv('Texas_cleaned.csv')
texas <- texas[-2]
texas <- texas[-3]
texas <- texas[-3]
texas <- texas[-6]
texas <- texas[-6]
texas <- texas[-1]
texas <- unique(texas)
washington <- washington[-2]
washington <- washington[-3]
washington <- washington[-3]
washington <- washington[-6]
washington <- washington[-6]
washington <- washington[-1]
washington <- unique(washington)
seattle <- washington[grepl('seattle', washington$user.location, ignore.case = TRUE),]
washington.other <- washington[!grepl('seattle', washington$user.location, ignore.case = TRUE) &
(grepl('WA', washington$user.location) |
grepl('washington', washington$user.location, ignore.case = TRUE)), ]
austin <- texas[grepl('austin', texas$user.location, ignore.case = TRUE),]
dallas <- texas[grepl('dallas', texas$user.location, ignore.case = TRUE),]
houston <- texas[grepl('houston', texas$user.location, ignore.case = TRUE),]
texas.other <- texas[! (grepl('austin', texas$user.location, ignore.case = TRUE) |
grepl('dallas', texas$user.location, ignore.case = TRUE) |
grepl('houston', texas$user.location, ignore.case = TRUE)) &
(grepl('TX', texas$user.location) |
grepl('texas', texas$user.location, ignore.case = TRUE)), ]
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
library(XML)
library(tm)
library(topicmodels)
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
seattle.text <- VCorpus(DataframeSource(as.data.frame(seattle$author.text)))
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean.tf = DocumentTermMatrix(seattle.text.clean, control = list(weighting = weightTf))
seattle.text.clean.tf
seattle.text.clean
seattle.text.clean$`3`
seattle.text.clean = tm_map(seattle.text, stripWhitespace)                          # remove extra whitespace
seattle.text.clean = tm_map(seattle.text.clean, removeNumbers)                      # remove numbers
seattle.text.clean = tm_map(seattle.text.clean, removePunctuation)                  # remove punctuation
seattle.text.clean = tm_map(seattle.text.clean, content_transformer(tolower))       # ignore case
seattle.text.clean = tm_map(seattle.text.clean, removeWords, stopwords("english"))  # remove stop words
seattle.text.clean = tm_map(seattle.text.clean, stemDocument, lazy = TRUE)                       # stem all words
seattle.text.clean$`3`
news = as.data.frame(xmlToDataFrame("Data/news_documents.xml", stringsAsFactors = FALSE)[,"c"])
news = as.data.frame(xmlToDataFrame("~/MSDS/SYS6018/Examples/Data/news_documents.xml", stringsAsFactors = FALSE)[,"c"])
news = VCorpus(DataframeSource(news))
# inspect the first two documents (sub-corpus)
inspect(news[1:2])
# view content of first document
news[[1]]$content
# compute TF-IDF matrix
news.tfidf = DocumentTermMatrix(news, control = list(weighting = weightTfIdf))
# clean up the corpus
news.clean = tm_map(news, stripWhitespace)                          # remove extra whitespace
news.clean = tm_map(news.clean, removeNumbers)                      # remove numbers
news.clean = tm_map(news.clean, removePunctuation)                  # remove punctuation
news.clean = tm_map(news.clean, content_transformer(tolower))       # ignore case
news.clean = tm_map(news.clean, removeWords, stopwords("english"))  # remove stop words
news.clean = tm_map(news.clean, stemDocument)
inspect(news.clean[1:2])
news.clean[[1]]$content
seattle.text.clean[[1]]$content
str_count(seattle.text.clean[[1]]$content)
library(stringr)
str_count(seattle.text.clean[[1]]$content)
